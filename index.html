<!doctype html>
<html>
  <head>
    <title>Tech and travel</title>
    <meta http-equiv="content-type" content="text/html;charset=utf-8" />
    <link rel="stylesheet" type="text/css" href="style.css">
  </head>
  <body>
    <div id="header">
      <div class="container">
        <a id="header-title" href="index.html">My Blog</a>
        <ul id="header-nav">
          <li><a href="about.html">About</a></li>
          <li><a href="mailto:soniagrunenwald@gmail.com">Contact</a></li>
        </ul>
      </div>
    </div>
    <div id="content">
      <div class="post-container">
        <div class="post">
          <div class="post-author">
            <img src="ID.jpg" width="200" height="300">
            <span>Sonia Grunenwald</span>
          </div>
          <p class="post-date">28th November 2018</p>
          <h3 class="post-title">Why cryonics is not just a matter of personal decision</h3>
          <div class="post-content">
            <p>In 2016, the first teenager British girlhas been cryogenically frozen after prematurely dying from cancer, raising the debate between pro-cryonics and opponents. Cryonics… It seems like science fiction but it is happening now. Freezing your body for an undetermined period of time until humans find a cure against death and allow us to live forever? It sounds full of promises. The concept is simple. You pay between $28,000 and $200,000 for a team of scientists to keep your frozen in the hope that, someday, a new team of scientists will find a way to resuscitate you. People and insurance companies invest in it; some scientists dedicate their lives to it… It would be the ultimate technological advancement to finally defeat death itself.</p>
            <p>Nevertheless, no matter how full of promises this technology is, it is indeed only a promise for better time, and is certainly not a life-changing technology in itself. Preserving bodies in different ways has been done since the Antiquity in with Egyptians pharaohs and wives. The only substantial content of this technology is a hyped-up expectation for a potential future, for future generations to find out how to bring people back from the dead… It is, essentially, solely a waiting room. But expectations could have unprecedented consequences. So why should you sign up?</p>
            <p>The main argument highlighted by David Shaw, in favour of cryonics, is an updated version of Pascal’s wager. It suggests that we should bet that cryonics will work. If it doesn’t, we won’t live to regret it. If it works we could obtain the ultimate profit of living again and of potentially becoming immortal, outweighing all the financial costs of getting cryogenically frozen. On the other hand, by betting that cryonics will not work, you might miss the opportunity of living forever in exchange of saving some money.</p>
            <p>However, David Shaw presents choosing cryonics as a personal decision… But it might impact more than yourself, and this is why we should care and think about it.</p>
            <p>There are numerous personal downsides, such as not living fully our lives to invest in the hope of a future life, or the risk of not preserving our persona when being revived etc. But these are a matter of personal decisions and beliefs, so we will not focus on them, but rather on the social impacts of ‘betting’ in favour of cryonics.</p>
            <p>Indeed, a normalisation of cryonics will have considerable impacts on ethical consideration and policy making. Indeed, the implications for life insurance or the future of the family would be very significant. Policy makers will need to think about the nature of the status of a cryonaut. Will they be considered dead, alive, or in-between? That cryonics work will then be a matter of international concern for law rather than a matter of personal opinion. After deciding on the status of cryonauts, policy makers will have to regulate the future of the estate of the frozen ones. Will they keep their estates in the hope of revival, or will the family inherit? It will further impact family planning. If a women’s husband gets cryonics, is she considered a widow and allowed to get married again? If policy remains unchanged but the hype for cryonics and revival keeps on rising, the ethical and social issues arising would be unprecedented. A person believing that he will get revived might be more prone to commit suicide in order to get rid of his or her debts, or in order to allow his or her family to access his life insurance. People might want to help their relatives die sooner, with less pain, believing that they can be revived again. People might want to die younger to enjoy being revived in a young and functioning body, and so on.</p>
            <p>Further, cryonics promise that, in the future, someone will be able to bring cryonauts back to life, displaying high expectations for the future of science and the future generations. But numerous legal questions remain… If no one finds a cure for death, how long does the cryonics companies keep a body before giving up? Indeed, can we ethically use that much energy (the bodies need to be kept at -130°C temperatures) to preserve the dead when the livings are struggling with energy limitations and global warming? Consequently, the environmental costs of the process might lead governments to forbid it, if our planet’s temperature keep on rising. Further, the significant financial costs of preserving the bodies might make the process unprofitable in the long run, and various institutes offering such services could go bankrupt. Then, if the enterprise is aborted, what do we do with the bodies and what are the legal implications for the companies failing to deliver the promised service?</p>
            <p>Policy makers should also consider the impacts cryonics will have on future generations. Circumstances under which future generations might not find a cure against death need to be legally framed, questioning the consequences for the cryonauts. Alternatively, they should consider the cases where future generations can, but refuse to revive the bodies. Indeed, reviving preserved corpses from the past could bring back diseases and viruses that future generations are not protected against. Or Earth could be over-populated and there would not be room to bring back the dead. Or bringing them back might be too expensive, and might implicate too much efforts to help the cryonauts adapt to the new era they are now living in. Finally, if future generations revive the cryogenically frozen corpses, policies should frame their future lives and consider how to integrate them to the new society. Family legal considerations will once more be at stake. For example, Robert Ettinger, got cryogenically frozen with his two wives. Which one will be considered his wife when he will be revived?  And what about the cryonauts who got frozen too young to pursue a life on their own, such as the fourteen years old aforementioned? What age will she legally be, and how should she be taken care of?</p>
            <p>There is an infinite amount of other scenarios to consider when developing a technology such as cryonics. The technology is, hence, not solely a matter of personal decision. No matter whether cryonauts are revived or not, the moment the technology becomes widely spread, it will have significant social and environmental consequences on our current world. If cryonauts can be revived, then there will be significant consequences on the future generations. Cryonics need, thus, to be thought about and regulated from today, prior to discovering how to revive the frozen bodies.</p>
          </div>
        </div>
      </div>
      <div class="post-container">
        <div class="post">
          <div class="post-author">
            <img src="ID.jpg" width="200" height="300">
            <span>Sonia Grunenwald</span>
          </div>
          <p class="post-date">14th January 2019</p>
          <h3 class="post-title">Netflix controls what we see and this is why it matters</h3>
          <div class="post-content">
            <p> Sunday evening, it is cold and rainy, you are cuddled up in your blanket, you log on Netflix and at once find a movie you’re interested in.</p>
            <P>What really happens is that <a href="https://neilpatel.com/blog/how-netflix-uses-analytics/">Netflix collects a ton of data</a> about yourself, your taste and your viewing behaviour, through seemingly inoffensive “cookies”. It then analyses it, implements it in complex algorithms and personalizes your user board to only show you relevant suggestions and make you happier on Netflix (and to make sure you don’t cancel your subscription). The same phenomenon also happens on almost every website that we use like Facebook, Twitter, Google etc.</p>
            <p>But what is relevant? According to these recommender-algorithms, relevance is what we want to see and know. It is the link we’ll be more likely to click on. By using our personal data, websites can adjust everything to us and make sure we find what we want to find. Everything matches our personal preferences and needs.</p>
            <P>This is how it works. A recommender-algorithm, such as the one used by Netflix, is built upon three main features: content-based similarity, collaborative similarity and popularity. The first makes Netflix’s algorithm suggest another movie that is believed to have similar content to the previous movies we liked (depending on the grade we gave it). Collaborative similarity is used when user A and user B both like a number of similar movies, the algorithm will then suggest movies to user A that user B liked and vice versa. Popularity corresponds to the general rate of a movie and how much the movie is talked about. Indeed, if we had several people or website recommending us a movie, we are more likely to watch it.</p>
            <p>Since recommender-algorithms are used constantly in our everyday life, let us critically review the impacts of their design on our personal development and on the world we are constructing with them.</p>
            <p>We like how websites are customized to our personal needs. We believe that we have some control over this by modifying our data preferences. We are aware that our personal data is used to tailor our user interfaces and browsers research. But permanent relevance of our search results, movies and music suggestions builds up our digital comfort zone. We only see things we are at ease with and are consequently stuck in a technological lock-in. Why would we cancel our Netflix subscription for an alternative streaming platform when Netflix always comes up with a movie we like?</p>
            <p>The technological lock-in even occurs while remaining on Netflix. By being comforted in our behaviours, we become more reluctant to change. Suppose that you particularly enjoy watching superheroes movies on Netflix but also wish to learn more about high quality independent movies. Your behaviour data tells Netflix that you’ll be more likely to click on “Avengers: Infinity War” than on “Battleship Potemkin”. Consequently, your Netflix home page is more likely to be filled with Marvel movies than with mid-20s Russian movies, inhibiting your initial motivation. It creates a feedback loop that tells Netflix that the movie suggestion was both relevant and accurate, thus reducing the chances that Netflix will recommend “Battleship Potemkin” next time. If no user breaks this loop, then, according to the collaborative similarity feature of the algorithm, no one else with similar taste will be likely to have different suggestions on their Netflix dashboard.</p>
            <p>We, finally, get so used to see ‘relevant’ results that we start thinking that what we see is a complete overview of the reality. And this, has even broader consequences than simply inhibiting our personal development, especially when we observe the use of such recommender-algorithms on other platforms.</p>
            <p>In fact, we’re slowly getting unwillingly stuck in an echo chamber. We know that we already create one by bonding and interacting with people with the same beliefs and opinions as us, and we slowly restrict our conversations to these people. We start being convinced that most people think this way, until majority votes don’t match our perception.</p>
            <p>However, such misinterpretations do not only occur because of our social relationships, off and online. Data recommender-algorithms also contribute to provide us with a blurred view of reality. Indeed, taking Google’s example, by only finding results that are “relevant” to us according to a preference-based-algorithm, we might just obtain a very biased view of a certain topic.</p>
            <p>We become so used to seeing similar types of information and activities according to our preferences and normal activities that we are being isolated in our own world, our own bubble. This is what Eli Pariser calls the ‘<strong>filter bubble</strong>’ phenomenon. The first results we see when googling something, the looks of our Facebook or Instagram newsfeed etc., they are all tailored by recommender-algorithms based on our personal data. They give us relevant information, according to what we’ve been doing in the past. But, as Eli Praser puts it, “<em>the internet is showing us what it thinks we want to see, but not necessarily what we need to see.</em>”</p>
            <p>This phenomenon is an extreme version of the ‘confirmation bias’. We slowly stop challenging our assumptions because every result we see tends to confirm what we already believe.
            <p>Consequently, such filter-bubbles can boost political radicalization. A study on Facebook user’s online behaviouralready highlighted that we have a tendency to read articles and publications that support our ideologies, political views and positions. But when algorithms start erasing contradicting views from our newsfeed, or when Google only gives us article matching our political opinion, we don’t even have control over it anymore. We get comforted in our ideas without confronting them to others and are likely to believe that everyone thinks like you. By doing so, algorithms help creating groups of like-minded people who can easily radicalize to oppose themselves to the ‘others’.</p>
            <p>Further, biased information and matching activities can also reduce the diversity and accuracy of our knowledge as well as inhibit our creativity. Indeed, if similar user profiles are always presented with similar content, all the concerned profiles will have the same type of activity and knowledge, inhibiting creative thoughts. One of the key elements of finding good ideas, according to Stephen Johnson, is accidental connections, or ‘serendipity’. It’s by getting out of the routine and by looking at things from different points of view that creative ideas can emerge. But if an algorithm chooses shows us content based on our past behaviour, we are more likely to face constant similar content, thus hiding a whole variety of new options from us, preventing us from potentially developing a new interest or having a new idea.</p>
            <p>Serendipitous recommendationsshould be both relevant, novel and unexpected. But we can understand why a company like Netflix would be reticent to implementing serendipity in its recommender-algorithms. On the one hand, researchhas been made on the usage of recommender-algorithms to provide suggestions of serendipitous movies, proving that, viewers were likely to enjoy a movie more when the suggestion was serendipitous. But will this extra margin of the user’s happiness translate into a significant profit for Netflix? On the other hand, suggesting a serendipitous movie to a user also represents a higher risk that the user won’t like the movie, translating as a significant cost for Netflix. If the user starts believing that Netflix doesn’t have relevant movies anymore, he might start thinking about changing streaming platform for a cheaper one.</p>
            <p>We like relevance, it saves us time and catches our interest. But it also corners us in a bubble of homogeneous thoughts. So perhaps platforms like Netflix, Google and Facebook should incorporate new preference settings allowing us to add serendipitous content to our content suggestions. In the meantime, we need to be aware of the presence of such filter bubbles that occur online, everyday. Biases are primarily induced by our activity behaviour and by the echo-chambers we are creating for ourselves, so everyday actions might enable us to expand our suggestions and horizons. For example, we can expand our research, modify our daily online activities, take time to interact with people from different points of viewboth on- and offline or take more control over our data settings.</p>
          </div>
        </div>
      </div>
    </div>
    <div id="footer">
      <div class="container">
        <div class="column">
          <h4>My Links</h4>
          <p>
            <a href="https://www.facebook.com/sonia.grunenwald">Facebook</a><br>
            <a href="https://www.linkedin.com/in/sonia-grunenwald/">LinkedIn</a><br>
            <a href="https://www.instagram.com/soniagrun/">Instagram</a><br>
            <a href="https://twitter.com/soniagrunenwald">Twitter</a>
          </p>
        </div>
        <div class="column">
          <h4>My Story</h4>
          <p>Hi there! I'm Sonia.</p>
        </div>
    </div>
  </body>
</html>
